---
title: "kNN, Exploration and Prediction of Health Indicators in Relation to Diabetes: A Practical Approach "
author: "Angie Tatiana Daza Malagón"
date: "`r Sys.Date()`"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## *Exploration and Prediction of Health Indicators in Relation to Diabetes: A Practical Approach*

### Introduction

In this exercise, we will embark on a predictive analysis concerning health indicators and diabetes by employing the k-Nearest Neighbors (KNN) algorithm. Our primary objective is to elucidate the connection between various health indicators and the presence of diabetes. We will embark on a comprehensive exploration of the dataset, prepare the data for machine learning, and proceed to train a KNN model for predictive purposes. Below, we will guide you through the code, dissecting it step by step.

## Part 1: Exploration and Data Manipulation

```{r  message=FALSE}
library(tidyverse)
library(caret)
library(MASS)
library(glmnet)
library(boot)
 
```

• **install.packages(c("caret", "MASS", "glmnet", "boot"))** installs the specified R packages if they are not already installed. These packages are essential for various data analysis tasks.

• **library(tidyverse)** loads the tidyverse package, which is a collection of R packages designed for data manipulation and visualization. It includes packages like ggplot2 and dplyr.

• **library(caret), library(MASS), library(glmnet), and library(boot)** load the respective packages into the R environment. These packages are used for machine learning, statistical modeling, and bootstrapping techniques.

#### Reading the Dataset:

```{r  message=FALSE}

data <- read.csv("diabetes_012_health_indicators_BRFSS2015 (2).csv")
 
```

**read.csv("diabetes_012_health_indicators_BRFSS2015 (2).csv")** reads the CSV (Comma-Separated Values) file named "diabetes_012_health_indicators_BRFSS2015.csv" into an R dataframe named data. This dataset likely contains health-related indicators, possibly including information related to diabetes, which will be used for further analysis in the subsequent parts of the code.

## Part 2: Data Sampling and Preparation

#### 1.Setting Seed and Sampling Data:

```{r  message=FALSE}

set.seed(123)
sampled_data <- data %>% sample_frac(0.01)
```

• **set.seed(123):** Sets the random seed to 123. This ensures that if you run the code again, you will get the same random results. It's useful for reproducibility in data analysis.

• **data %\>% sample_frac(0.01):** Takes a random 1% sample of the original dataset (data). The %\>% operator, also known as the pipe operator, is used here to pipe the output of the previous command (data) into the sample_frac function, which then samples 1% of the data.

#### 2. Creating Binary Diabetes Labels:

```{r  message=FALSE}

sampled_data$DiabetesBinary <- make.names(factor(ifelse(sampled_data$Diabetes_012 > 0, 1, 0)))
sampled_data$DiabetesBinary <- factor(sampled_data$DiabetesBinary, levels = c("X0", "X1"))
```

• **make.names(factor(ifelse(sampled_data\$Diabetes_012 \> 0, 1, 0))):** Converts the numeric variable Diabetes_012 into a binary factor variable. If Diabetes_012 is greater than 0, it's set to 1, otherwise 0. factor converts these numeric values into factors.

• **sampled_data\$DiabetesBinary \<- factor(\...):** Assigns the resulting factors to a new column called DiabetesBinary in the sampled_data dataframe.

• **factor(\..., levels = c("X0", "X1")):** Specifies the levels of the factor. Here, "X0" represents the absence of diabetes (when Diabetes_012 is 0) and "X1" represents the presence of diabetes (whenDiabetes_012 is greater than 0).

#### 3. Creating Subsets:

```{r  message=FALSE}

sampled_data_bmi <- sampled_data
sampled_data_menthlth <- sampled_data
sampled_data_physhtlth <- sampled_data
```

These lines create three new dataframes (**sampled_data_bmi, sampled_data_menthlth, and sampled_data_physhtlth**),each containing the same data as **sampled_data**. These subsets are likely created to perform specific analyses or modeling tasks on different health indicators while keeping the original sampled data intact for reference.

## Part 3: K-Nearest Neighbors (KNN) Classification

#### 1.Setting Seed and Creating Training and Test Sets:

```{r  message=FALSE}

set.seed(123)
train_index <- createDataPartition(sampled_data$DiabetesBinary, p = 0.8, list = FALSE, times = 1)
train_data <- sampled_data[train_index, ]
test_data <- sampled_data[-train_index, ]

```

**• set.seed(123):** Sets the random seed to 123 for reproducibility.

• **createDataPartition(sampled_data\$DiabetesBinary, p = 0.8, list = FALSE, times = 1):**Splits the sampled_data into training and test sets. 80% of the data is used for training (train_data),and 20% is used for testing (test_data). The indices for the training set are stored in train_index.

#### 2. Setting up Control Parameters for KNN Model:

```{r  message=FALSE}
sampled_data_bmi <- sampled_data
sampled_data_menthlth <- sampled_data
sampled_data_physhtlth <- sampled_data
ctrl <- trainControl(method = "cv", number = 10, classProbs = TRUE, summaryFunction = twoClassSummary)

```

**trainControl(...):** Configures the control parameters for the model training process. In this case, 10-fold cross-validation (method = "cv") is used with class probabilities enabled (classProbs = TRUE). The twoClassSummary function is specified for summarizing the results for binary classification.

#### 3.Training and Evaluating K-Nearest Neighbors (KNN) Model:

```{r  message=FALSE}
set.seed(20)
knn_model <- train(DiabetesBinary ~ ., data = train_data, method = "knn", trControl = ctrl, tuneLength = 10, metric = "Accuracy")
                   
```

```{r  message=TRUE}
predictions_knn <- predict(knn_model, newdata = test_data)
confusionMatrix(predictions_knn, test_data$DiabetesBinary)

```

• **set.seed(20):** Sets a different random seed (20) for the KNN model training process.

• **train(...):** Trains the KNN model. DiabetesBinary \~ . indicates that the column DiabetesBinary is the target variable, and all other columns are used as features. The training data is train_data, and the control parameters are specified by trControl.

• **predict(...):** Uses the trained knn_model to make predictions on the test data (test_data).

• **confusionMatrix(...):** Computes the confusion matrix to evaluate the performance of the KNN model by comparing the predicted values (predictions_knn) with the actual values (test_data\$DiabetesBinary).

```{r  message=FALSE}
print(knn_model)
```

**print(knn_model):** This line of code prints the details of the knn_model, which was trained using the train function. When you print a trained model in R, it shows various information about the model, including its parameters, training performance, and tuning results (if applicable). This information is crucial for understanding the characteristics of the trained model.

#### Part 4: Linear and Multilinear Regression

#### 1.Linear Regression Model for BMI:

```{r  message=FALSE}
model_bmi <- lm(BMI ~ ., data = sampled_data_bmi)
```

• **lm(BMI \~ ., data = sampled_data_bmi):** Fits a linear regression model where BMI is the dependent variable, and . represents that all other columns in the sampled_data_bmi dataframe are used as independent variables. This means the model is trying to predict BMI based on other available variables in the dataset.

#### 2.Cross-Validation for the BMI Regression Model:

```{r  message=FALSE}
cv_results_bmi <- cv.glm(data = sampled_data_bmi, glmfit = model_bmi, K = 10)
```

**cv.glm(data = sampled_data_bmi, glmfit = model_bmi, K = 10):** Performs 10-fold cross-validation (K= 10) using the linear regression model (glmfit = model_bmi) on the data in sampled_data_bmi. Crossvalidation is a technique used to assess how well a statistical model generalizes to an independent dataset.It involves dividing the dataset into K subsets, training the model on K-1 of the folds, and testing it on the remaining fold. This process is repeated K times, with each of the folds used exactly once as the validation data.

#### 3.Printing Cross-Validation Results:

```{r  message=FALSE}
print(cv_results_bmi)
```

```{r  message=FALSE}
cv_results_bmi
```

**print(cv_results_bmi):** Prints the results of cross-validation. This likely includes metrics such as mean squared error, mean absolute error, or other relevant statistics that indicate how well the regression model performs on the validation sets during the cross-validation process. Printing these results helps in understanding the performance of the model and comparing it against other models or variations of the same model.

#### 1.Linear Regression Model for Mental Health (MentHlth):

```{r  message=FALSE}
model_menthlth <- lm(MentHlth ~ ., data = sampled_data_menthlth)
```

**lm(MentHlth \~ ., data = sampled_data_menthlth):** This line of code creates a linear regression model where MentHlth is the dependent variable, and . indicates that all other columns in the sampled_data_menthlth dataframe are used as independent variables. The model aims to predict MentHlth based on other available variables in the dataset.

#### 2.Cross-Validation for the Mental Health Regression Model:

```{r  message=FALSE}
cv_results_menthlth <- cv.glm(data = sampled_data_menthlth, glmfit = model_menthlth, K = 10)

```

**cv.glm(data = sampled_data_menthlth, glmfit = model_menthlth, K = 10):** This line performs 10-fold cross-validation (K = 10) using the linear regression model (glmfit = model_menthlth) on the data in the sampled_data_menthlth dataframe. Cross-validation is a technique used to assess how well a statistical model generalizes to an independent dataset. It involves splitting the dataset into K subsets, training the model on K-1 of the folds, and testing it on the remaining fold. This process is repeated K times, with each fold used exactly once as the validation data.

#### 3.Printing Cross-Validation Results:

```{r  message=FALSE}
print(cv_results_menthlth)
```

```{r  message=FALSE}
cv_results_menthlth
```

**print(cv_results_menthlth):** This line prints the results of the cross-validation. The output likely includes metrics such as mean squared error, mean absolute error, or other relevant statistics indicating how well the regression model performs on the validation sets during the cross-validation process. Printing these results helps in understanding the performance of the model and comparing it against other models or variations of the same model.

1.  **Linear Regression Model for Physical Health (PhysHlth):**

```{r  message=FALSE}
model_physhtlth <- lm(PhysHlth ~ ., data = sampled_data_physhtlth)
```

**lm(PhysHlth \~ ., data = sampled_data_physhtlth):** This line creates a linear regression model where PhysHlth is the dependent variable, and . indicates that all other columns in the sampled_data_physhtlth dataframe are used as independent variables. The model attempts to predict PhysHlth based on other available variables in the dataset.

2.  **Cross-Validation for the Physical Health Regression Model:**

```{r  message=FALSE}
cv_results_physhtlth <- cv.glm(data = sampled_data_physhtlth, glmfit = model_physhtlth, K = 10)

```

**cv.glm(data = sampled_data_physhtlth, glmfit = model_physhtlth, K = 10):** This line performs 10-fold cross-validation (K = 10) using the linear regression model (glmfit = model_physhtlth) on the data in the sampled_data_physhtlth dataframe. Cross-validation is a technique used to assess how well a statistical model generalizes to an independent dataset. It involves splitting the dataset into K subsets, training the model on K-1 of the folds, and testing it on the remaining fold. This process is repeated K times, with each fold used exactly once as the validation data.

```{r  message=FALSE}
print(cv_results_physhtlth)

```

```{r  message=FALSE}
cv_results_physhtlth

```

**print(cv_results_physhtlth):** This line prints the results of the cross-validation. The output likely includes metrics such as mean squared error, mean absolute error, or other relevant statistics indicating how well the regression model performs on the validation sets during the cross-validation process. Printing these results helps in understanding the performance of the model and comparing it against other models or variations of the same model.

### CONCLUSIONS:

This analysis has addressed the relationship between health indicators and the presence of diabetes using various modeling techniques. We began with a robust data preparation and exploration phase, taking a representative sample for the study. Binary labels were created for diabetes, and subsets of data were extracted for specific health indicators such as BMI, Mental Health, and Physical Health.

For the classification of diabetes, we employed the K-Nearest Neighbors (KNN) algorithm, evaluating its performance through 10-fold cross-validation while testing various values of 'k'. Additionally, separate linear regression models were developed to predict the aforementioned health indicators. These models were evaluated using relevant metrics such as the mean squared error.Cross-validation played a fundamental role in this analysis, providing a more comprehensive view of the models' performance and their ability to generalize beyond the training data. Reproducibility was ensured by setting random seeds at critical points in the process, enhancing the reliability of the results and their replicability.

In summary, this analysis offers a comprehensive understanding of how health indicators are related to the presence of diabetes, utilizing both classification and regression approaches. These findings can have a significant impact on early identification and diabetes management, providing valuable insights for future research and clinical applications.
